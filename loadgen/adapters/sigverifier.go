/*
Copyright IBM Corp. All Rights Reserved.

SPDX-License-Identifier: Apache-2.0
*/

package adapters

import (
	"context"

	"github.com/cockroachdb/errors"
	"golang.org/x/sync/errgroup"
	"google.golang.org/grpc/credentials/insecure"
	"google.golang.org/protobuf/proto"

	"github.com/hyperledger/fabric-x-committer/api/protoblocktx"
	"github.com/hyperledger/fabric-x-committer/api/protosigverifierservice"
	"github.com/hyperledger/fabric-x-committer/api/types"
	"github.com/hyperledger/fabric-x-committer/loadgen/metrics"
	"github.com/hyperledger/fabric-x-committer/loadgen/workload"
	"github.com/hyperledger/fabric-x-committer/utils/connection"
)

type (
	// SvAdapter applies load on the SV.
	SvAdapter struct {
		commonAdapter
		config *VerifierClientConfig
	}
)

// NewSVAdapter instantiate SvAdapter.
func NewSVAdapter(config *VerifierClientConfig, res *ClientResources) *SvAdapter {
	return &SvAdapter{
		commonAdapter: commonAdapter{res: res},
		config:        config,
	}
}

// RunWorkload applies load on the SV.
func (c *SvAdapter) RunWorkload(ctx context.Context, txStream *workload.StreamWithSetup) error {
	updateMsg, err := createUpdate(c.res.Profile.Transaction.Policy)
	if err != nil {
		return errors.Wrap(err, "failed creating verification policy")
	}

	connections, err := connection.OpenConnections(c.config.Endpoints, insecure.NewCredentials())
	if err != nil {
		return errors.Wrap(err, "failed opening connections")
	}
	defer connection.CloseConnectionsLog(connections...)

	dCtx, dCancel := context.WithCancel(ctx)
	defer dCancel()
	g, gCtx := errgroup.WithContext(dCtx)
	streams := make([]protosigverifierservice.Verifier_StartStreamClient, len(connections))
	for i, conn := range connections {
		client := protosigverifierservice.NewVerifierClient(conn)

		logger.Infof("Opening stream to %s", c.config.Endpoints[i])
		streams[i], err = client.StartStream(gCtx)
		if err != nil {
			return errors.Wrapf(err, "failed opening a stream to %s", c.config.Endpoints[i])
		}

		logger.Infof("Set verification verification policy")
		err = streams[i].Send(&protosigverifierservice.RequestBatch{Update: updateMsg})
		if err != nil {
			return errors.Wrap(err, "failed submitting verification policy")
		}
	}

	for _, stream := range streams {
		g.Go(func() error {
			return sendBlocks(gCtx, &c.commonAdapter, txStream, c.mapToBatch, stream.Send)
		})
		g.Go(func() error {
			defer dCancel() // We stop sending if we can't track the received items.
			return c.receiveStatus(gCtx, stream)
		})
	}
	return errors.Wrap(g.Wait(), "workload done")
}

func createUpdate(policy *workload.PolicyProfile) (*protosigverifierservice.Update, error) {
	txSigner := workload.NewTxSignerVerifier(policy)

	envelopeBytes, err := workload.CreateConfigEnvelope(policy)
	if err != nil {
		return nil, errors.Wrap(err, "failed creating config")
	}
	updateMsg := &protosigverifierservice.Update{
		Config: &protoblocktx.ConfigTransaction{
			Envelope: envelopeBytes,
		},
		NamespacePolicies: &protoblocktx.NamespacePolicies{
			Policies: make([]*protoblocktx.PolicyItem, 0, len(txSigner.HashSigners)),
		},
	}

	for ns, p := range txSigner.HashSigners {
		if ns == types.MetaNamespaceID {
			continue
		}
		policyBytes, err := proto.Marshal(p.GetVerificationPolicy())
		if err != nil {
			return nil, errors.Wrap(err, "failed to serialize policy")
		}
		updateMsg.NamespacePolicies.Policies = append(
			updateMsg.NamespacePolicies.Policies,
			&protoblocktx.PolicyItem{
				Namespace: ns,
				Policy:    policyBytes,
			},
		)
	}

	return updateMsg, nil
}

func (c *SvAdapter) receiveStatus(
	ctx context.Context, stream protosigverifierservice.Verifier_StartStreamClient,
) error {
	for ctx.Err() == nil {
		responseBatch, err := stream.Recv()
		if err != nil {
			return errors.Wrap(connection.FilterStreamRPCError(err), "failed receiving verification status")
		}

		logger.Debugf("Received SV batch with %d responses", len(responseBatch.Responses))
		statusBatch := make([]metrics.TxStatus, len(responseBatch.Responses))
		for i, response := range responseBatch.Responses {
			logger.Debugf("Received Responses: %s", response.Status)
			statusBatch[i] = metrics.TxStatus{TxID: response.TxId, Status: response.Status}
		}
		c.res.Metrics.OnReceiveBatch(statusBatch)
		if c.res.isReceiveLimit() {
			return nil
		}
	}
	return nil
}

// mapToBatch creates a Verifier request batch. It uses the protoblocktx.Tx.Id to track the TXs latency.
func (c *SvAdapter) mapToBatch(txs []*protoblocktx.Tx) (*protosigverifierservice.RequestBatch, []string, error) {
	reqs := make([]*protosigverifierservice.Request, len(txs))
	for i, tx := range txs {
		reqs[i] = &protosigverifierservice.Request{
			BlockNum: c.NextBlockNum(),
			TxNum:    uint64(i), //nolint:gosec // int -> uint64.
			Tx:       tx,
		}
	}
	return &protosigverifierservice.RequestBatch{Requests: reqs}, getTXsIDs(txs), nil
}
